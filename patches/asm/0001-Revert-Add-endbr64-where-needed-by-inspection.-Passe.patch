From 5f7fafff280e6e17d7fafea20fda99762d74581f Mon Sep 17 00:00:00 2001
From: Brent Cook <busterb@gmail.com>
Date: Mon, 30 Oct 2023 21:59:32 -0500
Subject: [PATCH 1/4] Revert "Add endbr64 where needed by inspection.  Passes
 regresson tests."

This reverts commit e2118101444d3cf3cab87297b363cecd5357ae89.
---
 src/lib/libcrypto/aes/asm/aes-x86_64.pl           | 13 -------------
 src/lib/libcrypto/aes/asm/aesni-sha1-x86_64.pl    |  4 ----
 src/lib/libcrypto/aes/asm/aesni-x86_64.pl         | 15 ---------------
 src/lib/libcrypto/aes/asm/bsaes-x86_64.pl         | 14 --------------
 src/lib/libcrypto/aes/asm/vpaes-x86_64.pl         | 14 --------------
 src/lib/libcrypto/bn/arch/amd64/bignum_add.S      |  1 -
 src/lib/libcrypto/bn/arch/amd64/bignum_cmadd.S    |  1 -
 src/lib/libcrypto/bn/arch/amd64/bignum_cmul.S     |  1 -
 src/lib/libcrypto/bn/arch/amd64/bignum_mul.S      |  1 -
 .../libcrypto/bn/arch/amd64/bignum_mul_4_8_alt.S  |  1 -
 .../libcrypto/bn/arch/amd64/bignum_mul_8_16_alt.S |  1 -
 src/lib/libcrypto/bn/arch/amd64/bignum_sqr.S      |  1 -
 .../libcrypto/bn/arch/amd64/bignum_sqr_4_8_alt.S  |  1 -
 .../libcrypto/bn/arch/amd64/bignum_sqr_8_16_alt.S |  1 -
 src/lib/libcrypto/bn/arch/amd64/bignum_sub.S      |  1 -
 src/lib/libcrypto/bn/arch/amd64/word_clz.S        |  1 -
 src/lib/libcrypto/bn/asm/modexp512-x86_64.pl      |  5 -----
 src/lib/libcrypto/bn/asm/x86_64-mont.pl           |  3 ---
 src/lib/libcrypto/bn/asm/x86_64-mont5.pl          |  5 -----
 src/lib/libcrypto/camellia/asm/cmll-x86_64.pl     |  8 --------
 src/lib/libcrypto/md5/asm/md5-x86_64.pl           |  1 -
 src/lib/libcrypto/modes/asm/ghash-x86_64.pl       |  4 ----
 src/lib/libcrypto/rc4/asm/rc4-md5-x86_64.pl       |  6 ++----
 src/lib/libcrypto/rc4/asm/rc4-x86_64.pl           |  7 ++-----
 src/lib/libcrypto/sha/asm/sha1-x86_64.pl          |  4 ----
 src/lib/libcrypto/sha/asm/sha512-x86_64.pl        |  1 -
 src/lib/libcrypto/whrlpool/asm/wp-x86_64.pl       |  1 -
 src/lib/libcrypto/x86_64cpuid.pl                  |  2 --
 28 files changed, 4 insertions(+), 114 deletions(-)

diff --git a/src/lib/libcrypto/aes/asm/aes-x86_64.pl b/src/lib/libcrypto/aes/asm/aes-x86_64.pl
index 78ba20ca5..b7399b552 100755
--- a/src/lib/libcrypto/aes/asm/aes-x86_64.pl
+++ b/src/lib/libcrypto/aes/asm/aes-x86_64.pl
@@ -318,7 +318,6 @@ $code.=<<___;
 .type	_x86_64_AES_encrypt,\@abi-omnipotent
 .align	16
 _x86_64_AES_encrypt:
-	endbr64
 	xor	0($key),$s0			# xor with key
 	xor	4($key),$s1
 	xor	8($key),$s2
@@ -549,7 +548,6 @@ $code.=<<___;
 .type	_x86_64_AES_encrypt_compact,\@abi-omnipotent
 .align	16
 _x86_64_AES_encrypt_compact:
-	endbr64
 	lea	128($sbox),$inp			# size optimization
 	mov	0-128($inp),$acc1		# prefetch Te4
 	mov	32-128($inp),$acc2
@@ -595,7 +593,6 @@ $code.=<<___;
 .hidden	asm_AES_encrypt
 asm_AES_encrypt:
 AES_encrypt:
-	endbr64
 	push	%rbx
 	push	%rbp
 	push	%r12
@@ -887,7 +884,6 @@ $code.=<<___;
 .type	_x86_64_AES_decrypt,\@abi-omnipotent
 .align	16
 _x86_64_AES_decrypt:
-	endbr64
 	xor	0($key),$s0			# xor with key
 	xor	4($key),$s1
 	xor	8($key),$s2
@@ -1142,7 +1138,6 @@ $code.=<<___;
 .type	_x86_64_AES_decrypt_compact,\@abi-omnipotent
 .align	16
 _x86_64_AES_decrypt_compact:
-	endbr64
 	lea	128($sbox),$inp			# size optimization
 	mov	0-128($inp),$acc1		# prefetch Td4
 	mov	32-128($inp),$acc2
@@ -1197,7 +1192,6 @@ $code.=<<___;
 .hidden	asm_AES_decrypt
 asm_AES_decrypt:
 AES_decrypt:
-	endbr64
 	push	%rbx
 	push	%rbp
 	push	%r12
@@ -1297,7 +1291,6 @@ $code.=<<___;
 .type	AES_set_encrypt_key,\@function,3
 .align	16
 AES_set_encrypt_key:
-	endbr64
 	push	%rbx
 	push	%rbp
 	push	%r12			# redundant, but allows to share 
@@ -1323,7 +1316,6 @@ AES_set_encrypt_key:
 .type	_x86_64_AES_set_encrypt_key,\@abi-omnipotent
 .align	16
 _x86_64_AES_set_encrypt_key:
-	endbr64
 	mov	%esi,%ecx			# %ecx=bits
 	mov	%rdi,%rsi			# %rsi=userKey
 	mov	%rdx,%rdi			# %rdi=key
@@ -1569,7 +1561,6 @@ $code.=<<___;
 .type	AES_set_decrypt_key,\@function,3
 .align	16
 AES_set_decrypt_key:
-	endbr64
 	push	%rbx
 	push	%rbp
 	push	%r12
@@ -1669,7 +1660,6 @@ $code.=<<___;
 .hidden	asm_AES_cbc_encrypt
 asm_AES_cbc_encrypt:
 AES_cbc_encrypt:
-	endbr64
 	cmp	\$0,%rdx	# check length
 	je	.Lcbc_epilogue
 	pushfq
@@ -2561,7 +2551,6 @@ $code.=<<___;
 .type	block_se_handler,\@abi-omnipotent
 .align	16
 block_se_handler:
-	endbr64
 	push	%rsi
 	push	%rdi
 	push	%rbx
@@ -2620,7 +2609,6 @@ block_se_handler:
 .type	key_se_handler,\@abi-omnipotent
 .align	16
 key_se_handler:
-	endbr64
 	push	%rsi
 	push	%rdi
 	push	%rbx
@@ -2678,7 +2666,6 @@ key_se_handler:
 .type	cbc_se_handler,\@abi-omnipotent
 .align	16
 cbc_se_handler:
-	endbr64
 	push	%rsi
 	push	%rdi
 	push	%rbx
diff --git a/src/lib/libcrypto/aes/asm/aesni-sha1-x86_64.pl b/src/lib/libcrypto/aes/asm/aesni-sha1-x86_64.pl
index 879d16793..bafa906a0 100644
--- a/src/lib/libcrypto/aes/asm/aesni-sha1-x86_64.pl
+++ b/src/lib/libcrypto/aes/asm/aesni-sha1-x86_64.pl
@@ -89,7 +89,6 @@ $code.=<<___;
 .type	aesni_cbc_sha1_enc,\@abi-omnipotent
 .align	16
 aesni_cbc_sha1_enc:
-	endbr64
 	# caller should check for SSSE3 and AES-NI bits
 	mov	OPENSSL_ia32cap_P+0(%rip),%r10d
 	mov	OPENSSL_ia32cap_P+4(%rip),%r11d
@@ -133,7 +132,6 @@ $code.=<<___;
 .type	aesni_cbc_sha1_enc_ssse3,\@function,6
 .align	16
 aesni_cbc_sha1_enc_ssse3:
-	endbr64
 	mov	`($win64?56:8)`(%rsp),$inp	# load 7th argument
 	#shr	\$6,$len			# debugging artefact
 	#jz	.Lepilogue_ssse3		# debugging artefact
@@ -652,7 +650,6 @@ $code.=<<___;
 .type	aesni_cbc_sha1_enc_avx,\@function,6
 .align	16
 aesni_cbc_sha1_enc_avx:
-	endbr64
 	mov	`($win64?56:8)`(%rsp),$inp	# load 7th argument
 	#shr	\$6,$len			# debugging artefact
 	#jz	.Lepilogue_avx			# debugging artefact
@@ -1103,7 +1100,6 @@ $code.=<<___;
 .type	ssse3_handler,\@abi-omnipotent
 .align	16
 ssse3_handler:
-	endbr64
 	push	%rsi
 	push	%rdi
 	push	%rbx
diff --git a/src/lib/libcrypto/aes/asm/aesni-x86_64.pl b/src/lib/libcrypto/aes/asm/aesni-x86_64.pl
index 07d40a84a..e662fbc7c 100644
--- a/src/lib/libcrypto/aes/asm/aesni-x86_64.pl
+++ b/src/lib/libcrypto/aes/asm/aesni-x86_64.pl
@@ -242,7 +242,6 @@ $code.=<<___;
 .type	${PREFIX}_encrypt,\@abi-omnipotent
 .align	16
 ${PREFIX}_encrypt:
-	endbr64
 	movups	($inp),$inout0		# load input
 	mov	240($key),$rounds	# key->rounds
 ___
@@ -256,7 +255,6 @@ $code.=<<___;
 .type	${PREFIX}_decrypt,\@abi-omnipotent
 .align	16
 ${PREFIX}_decrypt:
-	endbr64
 	movups	($inp),$inout0		# load input
 	mov	240($key),$rounds	# key->rounds
 ___
@@ -286,7 +284,6 @@ $code.=<<___;
 .type	_aesni_${dir}rypt3,\@abi-omnipotent
 .align	16
 _aesni_${dir}rypt3:
-	endbr64
 	$movkey	($key),$rndkey0
 	shr	\$1,$rounds
 	$movkey	16($key),$rndkey1
@@ -331,7 +328,6 @@ $code.=<<___;
 .type	_aesni_${dir}rypt4,\@abi-omnipotent
 .align	16
 _aesni_${dir}rypt4:
-	endbr64
 	$movkey	($key),$rndkey0
 	shr	\$1,$rounds
 	$movkey	16($key),$rndkey1
@@ -377,7 +373,6 @@ $code.=<<___;
 .type	_aesni_${dir}rypt6,\@abi-omnipotent
 .align	16
 _aesni_${dir}rypt6:
-	endbr64
 	$movkey		($key),$rndkey0
 	shr		\$1,$rounds
 	$movkey		16($key),$rndkey1
@@ -442,7 +437,6 @@ $code.=<<___;
 .type	_aesni_${dir}rypt8,\@abi-omnipotent
 .align	16
 _aesni_${dir}rypt8:
-	endbr64
 	$movkey		($key),$rndkey0
 	shr		\$1,$rounds
 	$movkey		16($key),$rndkey1
@@ -531,7 +525,6 @@ $code.=<<___;
 .type	aesni_ecb_encrypt,\@function,5
 .align	16
 aesni_ecb_encrypt:
-	endbr64
 	and	\$-16,$len
 	jz	.Lecb_ret
 
@@ -837,7 +830,6 @@ $code.=<<___;
 .type	aesni_ccm64_encrypt_blocks,\@function,6
 .align	16
 aesni_ccm64_encrypt_blocks:
-	endbr64
 ___
 $code.=<<___ if ($win64);
 	lea	-0x58(%rsp),%rsp
@@ -2487,7 +2479,6 @@ $code.=<<___;
 .type	${PREFIX}_set_decrypt_key,\@abi-omnipotent
 .align	16
 ${PREFIX}_set_decrypt_key:
-	endbr64
 	sub	\$8,%rsp
 	call	__aesni_set_encrypt_key
 	shl	\$4,$bits		# rounds-1 after _aesni_set_encrypt_key
@@ -2538,7 +2529,6 @@ $code.=<<___;
 .type	${PREFIX}_set_encrypt_key,\@abi-omnipotent
 .align	16
 ${PREFIX}_set_encrypt_key:
-	endbr64
 __aesni_set_encrypt_key:
 	sub	\$8,%rsp
 	mov	\$-1,%rax
@@ -2760,7 +2750,6 @@ $code.=<<___ if ($PREFIX eq "aesni");
 .type	ecb_se_handler,\@abi-omnipotent
 .align	16
 ecb_se_handler:
-	endbr64
 	push	%rsi
 	push	%rdi
 	push	%rbx
@@ -2780,7 +2769,6 @@ ecb_se_handler:
 .type	ccm64_se_handler,\@abi-omnipotent
 .align	16
 ccm64_se_handler:
-	endbr64
 	push	%rsi
 	push	%rdi
 	push	%rbx
@@ -2822,7 +2810,6 @@ ccm64_se_handler:
 .type	ctr32_se_handler,\@abi-omnipotent
 .align	16
 ctr32_se_handler:
-	endbr64
 	push	%rsi
 	push	%rdi
 	push	%rbx
@@ -2858,7 +2845,6 @@ ctr32_se_handler:
 .type	xts_se_handler,\@abi-omnipotent
 .align	16
 xts_se_handler:
-	endbr64
 	push	%rsi
 	push	%rdi
 	push	%rbx
@@ -2900,7 +2886,6 @@ $code.=<<___;
 .type	cbc_se_handler,\@abi-omnipotent
 .align	16
 cbc_se_handler:
-	endbr64
 	push	%rsi
 	push	%rdi
 	push	%rbx
diff --git a/src/lib/libcrypto/aes/asm/bsaes-x86_64.pl b/src/lib/libcrypto/aes/asm/bsaes-x86_64.pl
index 7098ba27f..20e9e1f71 100644
--- a/src/lib/libcrypto/aes/asm/bsaes-x86_64.pl
+++ b/src/lib/libcrypto/aes/asm/bsaes-x86_64.pl
@@ -813,7 +813,6 @@ $code.=<<___;
 .type	_bsaes_encrypt8,\@abi-omnipotent
 .align	64
 _bsaes_encrypt8:
-	endbr64
 	lea	.LBS0(%rip), $const	# constants table
 
 	movdqa	($key), @XMM[9]		# round 0 key
@@ -878,7 +877,6 @@ $code.=<<___;
 .type	_bsaes_decrypt8,\@abi-omnipotent
 .align	64
 _bsaes_decrypt8:
-	endbr64
 	lea	.LBS0(%rip), $const	# constants table
 
 	movdqa	($key), @XMM[9]		# round 0 key
@@ -970,7 +968,6 @@ $code.=<<___;
 .type	_bsaes_key_convert,\@abi-omnipotent
 .align	16
 _bsaes_key_convert:
-	endbr64
 	lea	.Lmasks(%rip), $const
 	movdqu	($inp), %xmm7		# load round 0 key
 	lea	0x10($inp), $inp
@@ -1060,7 +1057,6 @@ $code.=<<___;
 .type	bsaes_enc_key_convert,\@function,2
 .align	16
 bsaes_enc_key_convert:
-	endbr64
 	mov	240($inp),%r10d		# pass rounds
 	mov	$inp,%rcx		# pass key
 	mov	$out,%rax		# pass key schedule
@@ -1075,7 +1071,6 @@ bsaes_enc_key_convert:
 .align	16
 bsaes_encrypt_128:
 .Lenc128_loop:
-	endbr64
 	movdqu	0x00($inp), @XMM[0]	# load input
 	movdqu	0x10($inp), @XMM[1]
 	movdqu	0x20($inp), @XMM[2]
@@ -1108,7 +1103,6 @@ bsaes_encrypt_128:
 .type	bsaes_dec_key_convert,\@function,2
 .align	16
 bsaes_dec_key_convert:
-	endbr64
 	mov	240($inp),%r10d		# pass rounds
 	mov	$inp,%rcx		# pass key
 	mov	$out,%rax		# pass key schedule
@@ -1123,7 +1117,6 @@ bsaes_dec_key_convert:
 .type	bsaes_decrypt_128,\@function,4
 .align	16
 bsaes_decrypt_128:
-	endbr64
 .Ldec128_loop:
 	movdqu	0x00($inp), @XMM[0]	# load input
 	movdqu	0x10($inp), @XMM[1]
@@ -1169,7 +1162,6 @@ $code.=<<___;
 .type	bsaes_ecb_encrypt_blocks,\@abi-omnipotent
 .align	16
 bsaes_ecb_encrypt_blocks:
-	endbr64
 	mov	%rsp, %rax
 .Lecb_enc_prologue:
 	push	%rbp
@@ -1371,7 +1363,6 @@ $code.=<<___;
 .type	bsaes_ecb_decrypt_blocks,\@abi-omnipotent
 .align	16
 bsaes_ecb_decrypt_blocks:
-	endbr64
 	mov	%rsp, %rax
 .Lecb_dec_prologue:
 	push	%rbp
@@ -1577,7 +1568,6 @@ $code.=<<___;
 .type	bsaes_cbc_encrypt,\@abi-omnipotent
 .align	16
 bsaes_cbc_encrypt:
-	endbr64
 ___
 $code.=<<___ if ($win64);
 	mov	48(%rsp),$arg6		# pull direction flag
@@ -1865,7 +1855,6 @@ $code.=<<___;
 .type	bsaes_ctr32_encrypt_blocks,\@abi-omnipotent
 .align	16
 bsaes_ctr32_encrypt_blocks:
-	endbr64
 	mov	%rsp, %rax
 .Lctr_enc_prologue:
 	push	%rbp
@@ -2107,7 +2096,6 @@ $code.=<<___;
 .type	bsaes_xts_encrypt,\@abi-omnipotent
 .align	16
 bsaes_xts_encrypt:
-	endbr64
 	mov	%rsp, %rax
 .Lxts_enc_prologue:
 	push	%rbp
@@ -2489,7 +2477,6 @@ $code.=<<___;
 .type	bsaes_xts_decrypt,\@abi-omnipotent
 .align	16
 bsaes_xts_decrypt:
-	endbr64
 	mov	%rsp, %rax
 .Lxts_dec_prologue:
 	push	%rbp
@@ -2966,7 +2953,6 @@ $code.=<<___;
 .type	se_handler,\@abi-omnipotent
 .align	16
 se_handler:
-	endbr64
 	push	%rsi
 	push	%rdi
 	push	%rbx
diff --git a/src/lib/libcrypto/aes/asm/vpaes-x86_64.pl b/src/lib/libcrypto/aes/asm/vpaes-x86_64.pl
index 8ff8d8602..3ffb1a303 100644
--- a/src/lib/libcrypto/aes/asm/vpaes-x86_64.pl
+++ b/src/lib/libcrypto/aes/asm/vpaes-x86_64.pl
@@ -82,7 +82,6 @@ $code.=<<___;
 .type	_vpaes_encrypt_core,\@abi-omnipotent
 .align 16
 _vpaes_encrypt_core:
-	endbr64
 	mov	%rdx,	%r9
 	mov	\$16,	%r11
 	mov	240(%rdx),%eax
@@ -173,7 +172,6 @@ _vpaes_encrypt_core:
 .type	_vpaes_decrypt_core,\@abi-omnipotent
 .align	16
 _vpaes_decrypt_core:
-	endbr64
 	mov	%rdx,	%r9		# load key
 	mov	240(%rdx),%eax
 	movdqa	%xmm9,	%xmm1
@@ -281,7 +279,6 @@ _vpaes_decrypt_core:
 .type	_vpaes_schedule_core,\@abi-omnipotent
 .align	16
 _vpaes_schedule_core:
-	endbr64
 	# rdi = key
 	# rsi = size in bits
 	# rdx = buffer
@@ -467,7 +464,6 @@ _vpaes_schedule_core:
 .type	_vpaes_schedule_192_smear,\@abi-omnipotent
 .align	16
 _vpaes_schedule_192_smear:
-	endbr64
 	pshufd	\$0x80,	%xmm6,	%xmm0	# d c 0 0 -> c 0 0 0
 	pxor	%xmm0,	%xmm6		# -> c+d c 0 0
 	pshufd	\$0xFE,	%xmm7,	%xmm0	# b a _ _ -> b b b a
@@ -499,7 +495,6 @@ _vpaes_schedule_192_smear:
 .type	_vpaes_schedule_round,\@abi-omnipotent
 .align	16
 _vpaes_schedule_round:
-	endbr64
 	# extract rcon from xmm8
 	pxor	%xmm1,	%xmm1
 	palignr	\$15,	%xmm8,	%xmm1
@@ -567,7 +562,6 @@ _vpaes_schedule_low_round:
 .type	_vpaes_schedule_transform,\@abi-omnipotent
 .align	16
 _vpaes_schedule_transform:
-	endbr64
 	movdqa	%xmm9,	%xmm1
 	pandn	%xmm0,	%xmm1
 	psrld	\$4,	%xmm1
@@ -606,7 +600,6 @@ _vpaes_schedule_transform:
 .type	_vpaes_schedule_mangle,\@abi-omnipotent
 .align	16
 _vpaes_schedule_mangle:
-	endbr64
 	movdqa	%xmm0,	%xmm4	# save xmm0 for later
 	movdqa	.Lk_mc_forward(%rip),%xmm5
 	test	%rcx, 	%rcx
@@ -680,7 +673,6 @@ _vpaes_schedule_mangle:
 .type	${PREFIX}_set_encrypt_key,\@function,3
 .align	16
 ${PREFIX}_set_encrypt_key:
-	endbr64
 ___
 $code.=<<___ if ($win64);
 	lea	-0xb8(%rsp),%rsp
@@ -729,7 +721,6 @@ $code.=<<___;
 .type	${PREFIX}_set_decrypt_key,\@function,3
 .align	16
 ${PREFIX}_set_decrypt_key:
-	endbr64
 ___
 $code.=<<___ if ($win64);
 	lea	-0xb8(%rsp),%rsp
@@ -783,7 +774,6 @@ $code.=<<___;
 .type	${PREFIX}_encrypt,\@function,3
 .align	16
 ${PREFIX}_encrypt:
-	endbr64
 ___
 $code.=<<___ if ($win64);
 	lea	-0xb8(%rsp),%rsp
@@ -827,7 +817,6 @@ $code.=<<___;
 .type	${PREFIX}_decrypt,\@function,3
 .align	16
 ${PREFIX}_decrypt:
-	endbr64
 ___
 $code.=<<___ if ($win64);
 	lea	-0xb8(%rsp),%rsp
@@ -877,7 +866,6 @@ $code.=<<___;
 .type	${PREFIX}_cbc_encrypt,\@function,6
 .align	16
 ${PREFIX}_cbc_encrypt:
-	endbr64
 	xchg	$key,$len
 ___
 ($len,$key)=($key,$len);
@@ -961,7 +949,6 @@ $code.=<<___;
 .type	_vpaes_preheat,\@abi-omnipotent
 .align	16
 _vpaes_preheat:
-	endbr64
 	lea	.Lk_s0F(%rip), %r10
 	movdqa	-0x20(%r10), %xmm10	# .Lk_inv
 	movdqa	-0x10(%r10), %xmm11	# .Lk_inv+16
@@ -1092,7 +1079,6 @@ $code.=<<___;
 .type	se_handler,\@abi-omnipotent
 .align	16
 se_handler:
-	endbr64
 	push	%rsi
 	push	%rdi
 	push	%rbx
diff --git a/src/lib/libcrypto/bn/arch/amd64/bignum_add.S b/src/lib/libcrypto/bn/arch/amd64/bignum_add.S
index 06298ca69..d56fa5e3a 100644
--- a/src/lib/libcrypto/bn/arch/amd64/bignum_add.S
+++ b/src/lib/libcrypto/bn/arch/amd64/bignum_add.S
@@ -49,7 +49,6 @@
 
 
 S2N_BN_SYMBOL(bignum_add):
-	endbr64
 
 #if WINDOWS_ABI
         push    rdi
diff --git a/src/lib/libcrypto/bn/arch/amd64/bignum_cmadd.S b/src/lib/libcrypto/bn/arch/amd64/bignum_cmadd.S
index 5ad712749..1dc1e5870 100644
--- a/src/lib/libcrypto/bn/arch/amd64/bignum_cmadd.S
+++ b/src/lib/libcrypto/bn/arch/amd64/bignum_cmadd.S
@@ -54,7 +54,6 @@
 
 
 S2N_BN_SYMBOL(bignum_cmadd):
-	endbr64
 
 #if WINDOWS_ABI
         push    rdi
diff --git a/src/lib/libcrypto/bn/arch/amd64/bignum_cmul.S b/src/lib/libcrypto/bn/arch/amd64/bignum_cmul.S
index 9199c8f48..c1a23ccea 100644
--- a/src/lib/libcrypto/bn/arch/amd64/bignum_cmul.S
+++ b/src/lib/libcrypto/bn/arch/amd64/bignum_cmul.S
@@ -51,7 +51,6 @@
 
 
 S2N_BN_SYMBOL(bignum_cmul):
-	endbr64
 
 #if WINDOWS_ABI
         push    rdi
diff --git a/src/lib/libcrypto/bn/arch/amd64/bignum_mul.S b/src/lib/libcrypto/bn/arch/amd64/bignum_mul.S
index 2d7ed1909..42ac988a1 100644
--- a/src/lib/libcrypto/bn/arch/amd64/bignum_mul.S
+++ b/src/lib/libcrypto/bn/arch/amd64/bignum_mul.S
@@ -59,7 +59,6 @@
 
 
 S2N_BN_SYMBOL(bignum_mul):
-	endbr64
 
 #if WINDOWS_ABI
         push    rdi
diff --git a/src/lib/libcrypto/bn/arch/amd64/bignum_mul_4_8_alt.S b/src/lib/libcrypto/bn/arch/amd64/bignum_mul_4_8_alt.S
index f02b09b28..3b7848b28 100644
--- a/src/lib/libcrypto/bn/arch/amd64/bignum_mul_4_8_alt.S
+++ b/src/lib/libcrypto/bn/arch/amd64/bignum_mul_4_8_alt.S
@@ -72,7 +72,6 @@
         adc     h, rdx
 
 S2N_BN_SYMBOL(bignum_mul_4_8_alt):
-	endbr64
 
 #if WINDOWS_ABI
         push    rdi
diff --git a/src/lib/libcrypto/bn/arch/amd64/bignum_mul_8_16_alt.S b/src/lib/libcrypto/bn/arch/amd64/bignum_mul_8_16_alt.S
index 97be83e1f..1be37840d 100644
--- a/src/lib/libcrypto/bn/arch/amd64/bignum_mul_8_16_alt.S
+++ b/src/lib/libcrypto/bn/arch/amd64/bignum_mul_8_16_alt.S
@@ -72,7 +72,6 @@
         adc     h, rdx
 
 S2N_BN_SYMBOL(bignum_mul_8_16_alt):
-	endbr64
 
 #if WINDOWS_ABI
         push    rdi
diff --git a/src/lib/libcrypto/bn/arch/amd64/bignum_sqr.S b/src/lib/libcrypto/bn/arch/amd64/bignum_sqr.S
index c4a0cabf3..2e05b9c17 100644
--- a/src/lib/libcrypto/bn/arch/amd64/bignum_sqr.S
+++ b/src/lib/libcrypto/bn/arch/amd64/bignum_sqr.S
@@ -62,7 +62,6 @@
 #define llshort ebp
 
 S2N_BN_SYMBOL(bignum_sqr):
-	endbr64
 
 #if WINDOWS_ABI
         push    rdi
diff --git a/src/lib/libcrypto/bn/arch/amd64/bignum_sqr_4_8_alt.S b/src/lib/libcrypto/bn/arch/amd64/bignum_sqr_4_8_alt.S
index b228414dc..a635177c6 100644
--- a/src/lib/libcrypto/bn/arch/amd64/bignum_sqr_4_8_alt.S
+++ b/src/lib/libcrypto/bn/arch/amd64/bignum_sqr_4_8_alt.S
@@ -71,7 +71,6 @@
         adc     c, 0
 
 S2N_BN_SYMBOL(bignum_sqr_4_8_alt):
-	endbr64
 
 #if WINDOWS_ABI
         push    rdi
diff --git a/src/lib/libcrypto/bn/arch/amd64/bignum_sqr_8_16_alt.S b/src/lib/libcrypto/bn/arch/amd64/bignum_sqr_8_16_alt.S
index 04efeec7e..f698202d2 100644
--- a/src/lib/libcrypto/bn/arch/amd64/bignum_sqr_8_16_alt.S
+++ b/src/lib/libcrypto/bn/arch/amd64/bignum_sqr_8_16_alt.S
@@ -103,7 +103,6 @@
         adc     c, 0
 
 S2N_BN_SYMBOL(bignum_sqr_8_16_alt):
-	endbr64
 
 #if WINDOWS_ABI
         push    rdi
diff --git a/src/lib/libcrypto/bn/arch/amd64/bignum_sub.S b/src/lib/libcrypto/bn/arch/amd64/bignum_sub.S
index 11a9bd7ed..f8e1fe35a 100644
--- a/src/lib/libcrypto/bn/arch/amd64/bignum_sub.S
+++ b/src/lib/libcrypto/bn/arch/amd64/bignum_sub.S
@@ -49,7 +49,6 @@
 
 
 S2N_BN_SYMBOL(bignum_sub):
-	endbr64
 
 #if WINDOWS_ABI
         push    rdi
diff --git a/src/lib/libcrypto/bn/arch/amd64/word_clz.S b/src/lib/libcrypto/bn/arch/amd64/word_clz.S
index 464a9d90f..025e98f9c 100644
--- a/src/lib/libcrypto/bn/arch/amd64/word_clz.S
+++ b/src/lib/libcrypto/bn/arch/amd64/word_clz.S
@@ -30,7 +30,6 @@
         .text
 
 S2N_BN_SYMBOL(word_clz):
-	endbr64
 
 #if WINDOWS_ABI
         push    rdi
diff --git a/src/lib/libcrypto/bn/asm/modexp512-x86_64.pl b/src/lib/libcrypto/bn/asm/modexp512-x86_64.pl
index af78fff54..2e71a7f03 100644
--- a/src/lib/libcrypto/bn/asm/modexp512-x86_64.pl
+++ b/src/lib/libcrypto/bn/asm/modexp512-x86_64.pl
@@ -347,7 +347,6 @@ $code.=<<___;
 .type	MULADD_128x512,\@abi-omnipotent
 .align	16
 MULADD_128x512:
-	endbr64
 ___
 	&MULSTEP_512([map("%r$_",(8..15))], "(+8*0)(%rcx)", "%rsi", "%rbp", "%rbx");
 $code.=<<___;
@@ -415,7 +414,6 @@ $code.=<<___;
 .type	mont_reduce,\@abi-omnipotent
 .align	16
 mont_reduce:
-	endbr64
 ___
 
 my $STACK_DEPTH         =  8;
@@ -678,7 +676,6 @@ $code.=<<___;
 .type	mont_mul_a3b,\@abi-omnipotent
 .align	16
 mont_mul_a3b:
-	endbr64
 	#
 	# multiply tmp = src1 * src2
 	# For multiply: dst = rcx, src1 = rdi, src2 = rsi
@@ -1080,7 +1077,6 @@ $code.=<<___;
 .type	sqr_reduce,\@abi-omnipotent
 .align	16
 sqr_reduce:
-	endbr64
 	 mov	(+$pResult_offset+8)(%rsp), %rcx
 ___
 	&SQR_512("%rsp+$tmp16_offset+8", "%rcx", [map("%r$_",(10..15,8..9))], "%rbx", "%rbp", "%rsi", "%rdi");
@@ -1110,7 +1106,6 @@ $code.=<<___;
 .globl	mod_exp_512
 .type	mod_exp_512,\@function,4
 mod_exp_512:
-	endbr64
 	 push	%rbp
 	 push	%rbx
 	 push	%r12
diff --git a/src/lib/libcrypto/bn/asm/x86_64-mont.pl b/src/lib/libcrypto/bn/asm/x86_64-mont.pl
index 6f5ab331e..cae7309d5 100755
--- a/src/lib/libcrypto/bn/asm/x86_64-mont.pl
+++ b/src/lib/libcrypto/bn/asm/x86_64-mont.pl
@@ -63,7 +63,6 @@ $code=<<___;
 .type	bn_mul_mont,\@function,6
 .align	16
 bn_mul_mont:
-	endbr64
 	test	\$3,${num}d
 	jnz	.Lmul_enter
 	cmp	\$8,${num}d
@@ -279,7 +278,6 @@ $code.=<<___;
 .align	16
 bn_mul4x_mont:
 .Lmul4x_enter:
-	endbr64
 	push	%rbx
 	push	%rbp
 	push	%r12
@@ -707,7 +705,6 @@ $code.=<<___;
 .align	16
 bn_sqr4x_mont:
 .Lsqr4x_enter:
-	endbr64
 	push	%rbx
 	push	%rbp
 	push	%r12
diff --git a/src/lib/libcrypto/bn/asm/x86_64-mont5.pl b/src/lib/libcrypto/bn/asm/x86_64-mont5.pl
index 3b3325a6c..7b9c6df27 100755
--- a/src/lib/libcrypto/bn/asm/x86_64-mont5.pl
+++ b/src/lib/libcrypto/bn/asm/x86_64-mont5.pl
@@ -57,7 +57,6 @@ $code=<<___;
 .type	bn_mul_mont_gather5,\@function,6
 .align	64
 bn_mul_mont_gather5:
-	endbr64
 	test	\$3,${num}d
 	jnz	.Lmul_enter
 	cmp	\$8,${num}d
@@ -388,7 +387,6 @@ $code.=<<___;
 .type	bn_mul4x_mont_gather5,\@function,6
 .align	16
 bn_mul4x_mont_gather5:
-	endbr64
 .Lmul4x_enter:
 	mov	${num}d,${num}d
 	movd	`($win64?56:8)`(%rsp),%xmm5	# load 7th argument
@@ -927,7 +925,6 @@ $code.=<<___;
 .type	bn_scatter5,\@abi-omnipotent
 .align	16
 bn_scatter5:
-	endbr64
 	cmp	\$0, $num
 	jz	.Lscatter_epilogue
 	lea	($tbl,$idx,8),$tbl
@@ -946,7 +943,6 @@ bn_scatter5:
 .type	bn_gather5,\@abi-omnipotent
 .align	16
 bn_gather5:
-	endbr64
 .LSEH_begin_bn_gather5:			# Win64 thing, but harmless in other cases
 	# I can't trust assembler to use specific encoding:-(
 	.byte	0x4c,0x8d,0x14,0x24			# lea    (%rsp),%r10
@@ -1057,7 +1053,6 @@ $code.=<<___;
 .type	mul_handler,\@abi-omnipotent
 .align	16
 mul_handler:
-	endbr64
 	push	%rsi
 	push	%rdi
 	push	%rbx
diff --git a/src/lib/libcrypto/camellia/asm/cmll-x86_64.pl b/src/lib/libcrypto/camellia/asm/cmll-x86_64.pl
index 3ceed3e89..586e5d6e9 100644
--- a/src/lib/libcrypto/camellia/asm/cmll-x86_64.pl
+++ b/src/lib/libcrypto/camellia/asm/cmll-x86_64.pl
@@ -116,7 +116,6 @@ $code=<<___;
 .type	Camellia_EncryptBlock,\@abi-omnipotent
 .align	16
 Camellia_EncryptBlock:
-	endbr64
 	movl	\$128,%eax
 	subl	$arg0d,%eax
 	movl	\$3,$arg0d
@@ -129,7 +128,6 @@ Camellia_EncryptBlock:
 .align	16
 .Lenc_rounds:
 Camellia_EncryptBlock_Rounds:
-	endbr64
 	push	%rbx
 	push	%rbp
 	push	%r13
@@ -178,7 +176,6 @@ Camellia_EncryptBlock_Rounds:
 .type	_x86_64_Camellia_encrypt,\@abi-omnipotent
 .align	16
 _x86_64_Camellia_encrypt:
-	endbr64
 	xor	0($key),@S[1]
 	xor	4($key),@S[0]		# ^=key[0-3]
 	xor	8($key),@S[3]
@@ -229,7 +226,6 @@ $code.=<<___;
 .type	Camellia_DecryptBlock,\@abi-omnipotent
 .align	16
 Camellia_DecryptBlock:
-	endbr64
 	movl	\$128,%eax
 	subl	$arg0d,%eax
 	movl	\$3,$arg0d
@@ -242,7 +238,6 @@ Camellia_DecryptBlock:
 .align	16
 .Ldec_rounds:
 Camellia_DecryptBlock_Rounds:
-	endbr64
 	push	%rbx
 	push	%rbp
 	push	%r13
@@ -291,7 +286,6 @@ Camellia_DecryptBlock_Rounds:
 .type	_x86_64_Camellia_decrypt,\@abi-omnipotent
 .align	16
 _x86_64_Camellia_decrypt:
-	endbr64
 	xor	0($key),@S[1]
 	xor	4($key),@S[0]		# ^=key[0-3]
 	xor	8($key),@S[3]
@@ -406,7 +400,6 @@ $code.=<<___;
 .type	Camellia_Ekeygen,\@function,3
 .align	16
 Camellia_Ekeygen:
-	endbr64
 	push	%rbx
 	push	%rbp
 	push	%r13
@@ -637,7 +630,6 @@ $code.=<<___;
 .type	Camellia_cbc_encrypt,\@function,6
 .align	16
 Camellia_cbc_encrypt:
-	endbr64
 	cmp	\$0,%rdx
 	je	.Lcbc_abort
 	push	%rbx
diff --git a/src/lib/libcrypto/md5/asm/md5-x86_64.pl b/src/lib/libcrypto/md5/asm/md5-x86_64.pl
index 06d69094f..c902a1b53 100755
--- a/src/lib/libcrypto/md5/asm/md5-x86_64.pl
+++ b/src/lib/libcrypto/md5/asm/md5-x86_64.pl
@@ -128,7 +128,6 @@ $code .= <<EOF;
 .globl md5_block_asm_data_order
 .type md5_block_asm_data_order,\@function,3
 md5_block_asm_data_order:
-	endbr64
 	push	%rbp
 	push	%rbx
 	push	%r12
diff --git a/src/lib/libcrypto/modes/asm/ghash-x86_64.pl b/src/lib/libcrypto/modes/asm/ghash-x86_64.pl
index 9ce0c3814..71d0822ac 100644
--- a/src/lib/libcrypto/modes/asm/ghash-x86_64.pl
+++ b/src/lib/libcrypto/modes/asm/ghash-x86_64.pl
@@ -412,7 +412,6 @@ $code.=<<___;
 .type	gcm_init_clmul,\@abi-omnipotent
 .align	16
 gcm_init_clmul:
-	endbr64
 	movdqu		($Xip),$Hkey
 	pshufd		\$0b01001110,$Hkey,$Hkey	# dword swap
 
@@ -450,7 +449,6 @@ $code.=<<___;
 .type	gcm_gmult_clmul,\@abi-omnipotent
 .align	16
 gcm_gmult_clmul:
-	endbr64
 	movdqu		($Xip),$Xi
 	movdqa		.Lbswap_mask(%rip),$T3
 	movdqu		($Htbl),$Hkey
@@ -478,7 +476,6 @@ $code.=<<___;
 .type	gcm_ghash_clmul,\@abi-omnipotent
 .align	16
 gcm_ghash_clmul:
-	endbr64
 ___
 $code.=<<___ if ($win64);
 .LSEH_begin_gcm_ghash_clmul:
@@ -689,7 +686,6 @@ $code.=<<___;
 .type	se_handler,\@abi-omnipotent
 .align	16
 se_handler:
-	endbr64
 	push	%rsi
 	push	%rdi
 	push	%rbx
diff --git a/src/lib/libcrypto/rc4/asm/rc4-md5-x86_64.pl b/src/lib/libcrypto/rc4/asm/rc4-md5-x86_64.pl
index 3190e6a8e..c65a2c751 100644
--- a/src/lib/libcrypto/rc4/asm/rc4-md5-x86_64.pl
+++ b/src/lib/libcrypto/rc4/asm/rc4-md5-x86_64.pl
@@ -38,7 +38,7 @@ my ($rc4,$md5)=(1,1);	# what to generate?
 my $D="#" if (!$md5);	# if set to "#", MD5 is stitched into RC4(),
 			# but its result is discarded. Idea here is
 			# to be able to use 'openssl speed rc4' for
-			# benchmarking the stitched subroutine... 
+			# benchmarking the stitched subroutine...
 
 my $flavour = shift;
 my $output  = shift;
@@ -109,7 +109,6 @@ $code.=<<___;
 .globl	$func
 .type	$func,\@function,$nargs
 $func:
-	endbr64
 	cmp	\$0,$len
 	je	.Labort
 	push	%rbx
@@ -405,7 +404,7 @@ $code.=<<___ if ($rc4 && (!$md5 || $D));
 	and	\$63,$len		# remaining bytes
 	jnz	.Loop1
 	jmp	.Ldone
-	
+
 .align	16
 .Loop1:
 	add	$TX[0]#b,$YY#b
@@ -454,7 +453,6 @@ $code.=<<___;
 .type	RC4_set_key,\@function,3
 .align	16
 RC4_set_key:
-	endbr64
 	lea	8($dat),$dat
 	lea	($inp,$len),$inp
 	neg	$len
diff --git a/src/lib/libcrypto/rc4/asm/rc4-x86_64.pl b/src/lib/libcrypto/rc4/asm/rc4-x86_64.pl
index 0472acce8..f678daaac 100755
--- a/src/lib/libcrypto/rc4/asm/rc4-x86_64.pl
+++ b/src/lib/libcrypto/rc4/asm/rc4-x86_64.pl
@@ -41,7 +41,7 @@
 
 # April 2005
 #
-# P4 EM64T core appears to be "allergic" to 64-bit inc/dec. Replacing 
+# P4 EM64T core appears to be "allergic" to 64-bit inc/dec. Replacing
 # those with add/sub results in 50% performance improvement of folded
 # loop...
 
@@ -127,9 +127,7 @@ $code=<<___;
 .globl	RC4
 .type	RC4,\@function,4
 .align	16
-RC4:
-	endbr64
-	or	$len,$len
+RC4:	or	$len,$len
 	jne	.Lentry
 	ret
 .Lentry:
@@ -435,7 +433,6 @@ $code.=<<___;
 .type	RC4_set_key,\@function,3
 .align	16
 RC4_set_key:
-	endbr64
 	lea	8($dat),$dat
 	lea	($inp,$len),$inp
 	neg	$len
diff --git a/src/lib/libcrypto/sha/asm/sha1-x86_64.pl b/src/lib/libcrypto/sha/asm/sha1-x86_64.pl
index e15ff47f8..43eee73c4 100755
--- a/src/lib/libcrypto/sha/asm/sha1-x86_64.pl
+++ b/src/lib/libcrypto/sha/asm/sha1-x86_64.pl
@@ -222,7 +222,6 @@ $code.=<<___;
 .type	sha1_block_data_order,\@function,3
 .align	16
 sha1_block_data_order:
-	endbr64
 	mov	OPENSSL_ia32cap_P+0(%rip),%r9d
 	mov	OPENSSL_ia32cap_P+4(%rip),%r8d
 	test	\$IA32CAP_MASK1_SSSE3,%r8d		# check SSSE3 bit
@@ -310,7 +309,6 @@ $code.=<<___;
 .align	16
 sha1_block_data_order_ssse3:
 _ssse3_shortcut:
-	endbr64
 	push	%rbx
 	push	%rbp
 	push	%r12
@@ -731,7 +729,6 @@ $code.=<<___;
 .align	16
 sha1_block_data_order_avx:
 _avx_shortcut:
-	endbr64
 	push	%rbx
 	push	%rbp
 	push	%r12
@@ -1102,7 +1099,6 @@ $code.=<<___;
 .type	se_handler,\@abi-omnipotent
 .align	16
 se_handler:
-	endbr64
 	push	%rsi
 	push	%rdi
 	push	%rbx
diff --git a/src/lib/libcrypto/sha/asm/sha512-x86_64.pl b/src/lib/libcrypto/sha/asm/sha512-x86_64.pl
index 120693fee..0517eab66 100755
--- a/src/lib/libcrypto/sha/asm/sha512-x86_64.pl
+++ b/src/lib/libcrypto/sha/asm/sha512-x86_64.pl
@@ -175,7 +175,6 @@ $code=<<___;
 .type	$func,\@function,4
 .align	16
 $func:
-	endbr64
 	push	%rbx
 	push	%rbp
 	push	%r12
diff --git a/src/lib/libcrypto/whrlpool/asm/wp-x86_64.pl b/src/lib/libcrypto/whrlpool/asm/wp-x86_64.pl
index 7958f6d28..de5d3acfb 100644
--- a/src/lib/libcrypto/whrlpool/asm/wp-x86_64.pl
+++ b/src/lib/libcrypto/whrlpool/asm/wp-x86_64.pl
@@ -57,7 +57,6 @@ $code=<<___;
 .type	$func,\@function,3
 .align	16
 $func:
-	endbr64
 	push	%rbx
 	push	%rbp
 	push	%r12
diff --git a/src/lib/libcrypto/x86_64cpuid.pl b/src/lib/libcrypto/x86_64cpuid.pl
index dc56732a2..1b67d1110 100644
--- a/src/lib/libcrypto/x86_64cpuid.pl
+++ b/src/lib/libcrypto/x86_64cpuid.pl
@@ -18,7 +18,6 @@ print<<___;
 .extern		OPENSSL_cpuid_setup
 .hidden		OPENSSL_cpuid_setup
 .section	.init
-	endbr64
 	call	OPENSSL_cpuid_setup
 
 .extern	OPENSSL_ia32cap_P
@@ -30,7 +29,6 @@ print<<___;
 .type	OPENSSL_ia32_cpuid,\@abi-omnipotent
 .align	16
 OPENSSL_ia32_cpuid:
-	endbr64
 	mov	%rbx,%r8		# save %rbx
 
 	xor	%eax,%eax
-- 
2.42.0

